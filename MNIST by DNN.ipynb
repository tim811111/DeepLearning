{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN train MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#import data and library\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "train_data = mnist.train.images  # Returns np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "test_data = mnist.test.images  # Returns np.array\n",
    "test_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "tf.logging.set_verbosity(old_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d347f8448bd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallow_growth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mInitilizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;31m#set SGD batch size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, op, message)\u001b[0m\n\u001b[0;32m    339\u001b[0m   \"\"\"\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m   \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m     \u001b[1;34m\"\"\"Creates a `ResourceExhaustedError`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m     super(ResourceExhaustedError, self).__init__(node_def, op, message,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#set outlines \n",
    "inputs_num = 784 # 28*28 =784  (784,1)\n",
    "outputs_num = 10 # 0 to 9\n",
    "hidden_layer_num = 3000 # 50 nodes per layer                        # 1\n",
    "\n",
    "#reset variable in tensorflow\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#set palceholder to input and output layer\n",
    "inputs= tf.placeholder(tf.float32,[None,inputs_num],)\n",
    "targets= tf.placeholder(tf.int32,[None,outputs_num],)\n",
    "\n",
    "#set first layer variable(xavier) and activation function\n",
    "w_1 = tf.get_variable('w_1',[inputs_num,hidden_layer_num])\n",
    "b_1 = tf.get_variable('b_1',[hidden_layer_num])\n",
    "\n",
    "output_1 = tf.nn.relu(tf.matmul(inputs,w_1)+b_1)                   # 2\n",
    "\n",
    "#set secod layer variable and activation function\n",
    "\n",
    "w_2 = tf.get_variable('w_2',[hidden_layer_num,hidden_layer_num])\n",
    "b_2 = tf.get_variable('b_2',[hidden_layer_num])\n",
    "\n",
    "output_2 = tf.nn.relu(tf.matmul(output_1,w_2)+b_2)                 # 2\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------\n",
    "# add hidden layer\n",
    "# w_3 = tf.get_variable('w_3',[hidden_layer_num,hidden_layer_num])\n",
    "# b_3 = tf.get_variable('b_3',[hidden_layer_num])\n",
    "\n",
    "# output_3 = tf.nn.sigmoid(tf.matmul(output_2,w_3)+b_3)\n",
    "\n",
    "# w_4 = tf.get_variable('w_4',[hidden_layer_num,hidden_layer_num])\n",
    "# b_4 = tf.get_variable('b_4',[hidden_layer_num])\n",
    "\n",
    "# output_4 = tf.nn.relu(tf.matmul(output_3,w_4)+b_4)\n",
    "\n",
    "# w_5 = tf.get_variable('w_5',[hidden_layer_num,hidden_layer_num])\n",
    "# b_5 = tf.get_variable('b_5',[hidden_layer_num])\n",
    "\n",
    "# output_5 = tf.nn.relu(tf.matmul(output_4,w_5)+b_5)\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------\n",
    "#set soutput layer variable and activation function\n",
    "\n",
    "w_3 = tf.get_variable('w_3',[hidden_layer_num,outputs_num])\n",
    "b_3 = tf.get_variable('b_3',[outputs_num])\n",
    "\n",
    "outputs = tf.matmul(output_2,w_3)+b_3\n",
    "\n",
    "\n",
    "#set loss function\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=outputs, labels=targets)   # 3 \n",
    "loss_mean = tf.reduce_mean(loss)\n",
    "\n",
    "#set optimization (Adam)\n",
    "optimize = tf.train.AdamOptimizer(learning_rate=0.00002,).minimize(loss_mean)       # 4\n",
    "\n",
    "#after softmax, let probability to 0 or 1 by argmax function, and judge between target and output\n",
    "\n",
    "out_eql_target = tf.equal(tf.argmax(outputs,1), tf.argmax(targets,1))\n",
    "\n",
    "#predicte accuracy (need cast to float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(out_eql_target,tf.float32))\n",
    "\n",
    "##框架完成\n",
    "\n",
    "\n",
    "#sess\n",
    "sess = tf.InteractiveSession()\n",
    "#Initialization variable\n",
    "Initilizer = tf.global_variables_initializer()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "sess.run(Initilizer)\n",
    "\n",
    "#set SGD batch size\n",
    "batch_size = 1000                                                                     #5\n",
    "batch_cycle = mnist.train._num_examples // batch_size\n",
    "\n",
    "#set early stopping\n",
    "max_epochs = 15\n",
    "\n",
    "prev_validation = 9999999.\n",
    "\n",
    "\n",
    "#training\n",
    "import time\n",
    "start = time.time()\n",
    "log_loss = []\n",
    "for epoch_counter in range(max_epochs):\n",
    "    curr_epoc_loss = 0.\n",
    "    for batch_counter in range(batch_cycle):\n",
    "        #set input and output batch(iterate by mnist.train.next_batch)\n",
    "        input_batch, target_batch = mnist.train.next_batch(batch_size)\n",
    "        _,batch_loss = sess.run([optimize,loss_mean], \n",
    "                               feed_dict={inputs : input_batch, targets : target_batch})\n",
    "        curr_epoc_loss += batch_loss\n",
    "    #read loss per epoch\n",
    "    curr_epoc_loss /= batch_cycle\n",
    "    \n",
    "    #validation every epoch to prevent overfitting()\n",
    "    input_batch, target_batch = mnist.validation.next_batch(mnist.validation._num_examples)\n",
    "    val_loss, val_accuracy = sess.run([loss_mean,accuracy],\n",
    "                                     feed_dict={inputs:input_batch,targets:target_batch})\n",
    "    if val_loss>prev_validation:\n",
    "        break\n",
    "    prev_validation = val_loss\n",
    "    \n",
    "    print(f'Epoch cycle: {epoch_counter + 1}'+\n",
    "         f' Training loss: {curr_epoc_loss:.3f}'+\n",
    "         f' Validation loss: {val_loss:.3f}'+\n",
    "         f' Validation accuracy: {val_accuracy*100.:.2f}'+'%'+\n",
    "         f' times: {time.time() - start}')\n",
    "    input_batch, target_batch = mnist.test.next_batch(mnist.test._num_examples)\n",
    "    test_accuracy = sess.run([accuracy],feed_dict ={inputs:input_batch,targets:target_batch})\n",
    "    test_accuracy_percent=test_accuracy[0]*100.\n",
    "    print(f'test accuracy: {test_accuracy_percent:.2f}')\n",
    "    log_loss.append((epoch_counter+1,curr_epoc_loss,val_loss,val_accuracy,time.time() - start))\n",
    "print('End of training')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tset\n",
    "\n",
    "input_batch, target_batch = mnist.test.next_batch(mnist.test._num_examples)\n",
    "test_accuracy = sess.run([accuracy],\n",
    "                         feed_dict ={inputs:input_batch,targets:target_batch})\n",
    "test_accuracy_percent=test_accuracy[0]*100.\n",
    "print(f'test accuracy: {test_accuracy_percent:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test Hyperparameter\n",
    "<br />\n",
    "2 layer\n",
    "<br />\n",
    "97.88  1000node  relu*2 layer    + softmax_cross_entropy  Learning_rate =0.001  batch=500 layer=5\n",
    "<br />\n",
    "<br />\n",
    "97.41  1000node  sigmoid*2 layer + softmax_cross_entropy  Learning_rate =0.001  batch=500 layer=5\n",
    "<br />\n",
    "<br />\n",
    "97.85  1000node  tahn*2 layer    + softmax_cross_entropy  Learning_rate =0.001  batch=500 layer=5\n",
    "<br />\n",
    "<br />\n",
    "97.9   1000node  relu*2 layer    + softmax_cross_entropy  Learning_rate =0.001  batch=100 layer=5\n",
    "<br />\n",
    "<br />\n",
    "97.99  2000node  relu*2 layer    + softmax_cross_entropy  Learning_rate =0.001  batch=100 layer=5\n",
    "<br />\n",
    "<br />\n",
    "97.49  2000node  relu*2 layer    + softmax_cross_entropy  Learning_rate =0.001  batch=1000 layer=5\n",
    "<br />\n",
    "<br />\n",
    "97.81  1000node  relu layer      + 2000node relu layer  + softmax_cross_entropy  Learning_rate =0.001  batch=1000 layer=5\n",
    "<br />\n",
    "<br />\n",
    "97.97  1000node  relu layer      + 2000node relu layer  + softmax_cross_entropy  Learning_rate =0.001  batch=1000 layer=5\n",
    "<br />\n",
    "<br />\n",
    "#### 98.05  3000node  relu*2 layer    + softmax_cross_entropy  Learning_rate =0.001  batch=1000 layer=5                       # best\n",
    "<br />\n",
    "<br />\n",
    "97.94  3000node  relu*2 layer    + softmax_cross_entropy  Learning_rate =0.001  batch=100 layer=5\n",
    "<br />\n",
    "<br />\n",
    "98.00 3000node  relu*2 layer    + softmax_cross_entropy  Learning_rate =0.0003  batch=1000 layer=5\n",
    "<br />\n",
    "<br />\n",
    "3 layer\n",
    "<br />\n",
    "97.99  1000node  relu*2 layer + sigmoid layer + softmax_cross_entropy  Learning_rate =0.001  batch=100 layer=5 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
